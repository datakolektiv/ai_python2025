{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86e4e71d",
   "metadata": {},
   "source": [
    "# GenAI/RAG in Python 2025\n",
    "\n",
    "## Session 05. The Foundations of Agentic AI\n",
    "\n",
    "- How an LLM can propose a plan and write follow-up prompts to itself (\"self-prompting\").\n",
    "- How to choose tools: either rely on the existing RAG (vectorized Italian recipes), or augment with Google Search when the RAG context looks weak or too narrow.\n",
    "- How to log every step (intent → tool decisions → results → final answer) for transparent inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04159b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from datetime import datetime\n",
    "import json\n",
    "import ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.spatial.distance import cosine\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f5d4d3",
   "metadata": {},
   "source": [
    "### 1. Programmable Search Engine (PSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01765e87",
   "metadata": {},
   "source": [
    "#### Create a Programmable Search Engine (PSE)\n",
    "\n",
    "- 1. Go to Google’s Programmable Search Engine and create a search engine. For a general web search agent, configure it to search the entire web (not just selected sites); the engine gives you a Search engine ID (cx). \n",
    "\n",
    "- 2. Enable the Custom Search JSON API in your Google Cloud project and create an API key (standard key is fine). \n",
    "\n",
    "- 3. Quota & pricing: Typical baseline has been ~100 free queries/day, then $5 per 1,000 queries (and a site-restricted variant without daily limit).\n",
    "\n",
    "Store credentials as env vars:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c066449",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"GOOGLE_CSE_API_KEY\"] = \"your API key here\"\n",
    "os.environ[\"GOOGLE_CSE_CX\"] = \"your PSE cx id here\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea325a3",
   "metadata": {},
   "source": [
    "#### Minimal Google Search client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5734f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "GOOGLE_CSE_API_KEY = os.environ[\"GOOGLE_CSE_API_KEY\"]\n",
    "GOOGLE_CSE_CX = os.environ[\"GOOGLE_CSE_CX\"]\n",
    "\n",
    "def google_search(query: str, num: int = 5):\n",
    "    \"\"\"\n",
    "    Calls Google's Custom Search JSON API and returns a list of {title, link, snippet}.\n",
    "    \"\"\"\n",
    "    url = \"https://www.googleapis.com/customsearch/v1\"\n",
    "    params = {\n",
    "        \"key\": GOOGLE_CSE_API_KEY,\n",
    "        \"cx\": GOOGLE_CSE_CX,\n",
    "        \"q\": query,\n",
    "        \"num\": min(max(num, 1), 10)  # API caps num<=10\n",
    "    }\n",
    "    r = requests.get(url, params=params, timeout=20)\n",
    "    r.raise_for_status()\n",
    "    data = r.json()\n",
    "    items = data.get(\"items\", []) or []\n",
    "    return [\n",
    "        {\"title\": it.get(\"title\"), \"link\": it.get(\"link\"), \"snippet\": it.get(\"snippet\")}\n",
    "        for it in items\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27322ef0",
   "metadata": {},
   "source": [
    "#### Test Google Search client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57645c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = \"ragù alla napoletana\"\n",
    "receipts = google_search(query = q, num = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0082fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "receipts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a244fcc",
   "metadata": {},
   "source": [
    "### 2. Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee517a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the embedding model to use (as per OpenAI docs)  \n",
    "model_name = \"text-embedding-3-small\"  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5634c2",
   "metadata": {},
   "source": [
    "### 3. OpenAI Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de138638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your API key (ensure OPENAI_API_KEY is set in your environment)\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Instantiate the OpenAI client with your API key  \n",
    "client = OpenAI(api_key=api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addbefbf",
   "metadata": {},
   "source": [
    "#### 3.1 Google Search Tool for our OpenAI Client\n",
    "\n",
    "We’ll expose google_search as a tool so the model can request it only when needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d513797d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"google_search\",\n",
    "            \"description\": \"Search the web for Italian cuisine info when RAG is insufficient.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"query\": {\"type\": \"string\", \"description\": \"Search query to send to Google\"},\n",
    "                    \"num\":   {\"type\": \"integer\", \"description\": \"How many results (1..10)\", \"minimum\": 1, \"maximum\": 10}\n",
    "                },\n",
    "                \"required\": [\"query\"]\n",
    "            },\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8710ccd3",
   "metadata": {},
   "source": [
    "#### 3.2 Google Search Tool dispatcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab09a8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tool_dispatch(tool_call):\n",
    "    if tool_call[0][\"function\"][\"name\"] == \"google_search\":\n",
    "        args = tool_call[0][\"function\"][\"parameters\"][\"properties\"]\n",
    "        # arguments arrives as a JSON string in chat.completions; parse it:\n",
    "        return google_search(args[\"query\"], args[\"num\"])\n",
    "    raise ValueError(f\"Unknown tool {tool_call[0][\"function\"][\"name\"]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5353e81e",
   "metadata": {},
   "source": [
    "### 4. Load Embeddings: Italian Recipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a06628",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = pd.read_csv(\"_data/italian_recipes_embedded.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68785aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c220a126",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(embeddings[\"embedding\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70f266c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Parse string embeddings into numpy arrays ---\n",
    "embeddings['embedding_vector'] = embeddings['embedding'].apply(\n",
    "    lambda x: np.array(ast.literal_eval(x), dtype=np.float32)\n",
    ")\n",
    "embeddings.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00892cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(embeddings[\"embedding_vector\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f635bb99",
   "metadata": {},
   "source": [
    "#### Similarity Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f836e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_retrieve(user_query: np.ndarray, \n",
    "                 top_k: int = 5, \n",
    "                 df: pd.DataFrame = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Retrieve top-k most similar recipes from an in-memory embeddings DataFrame\n",
    "    using cosine similarity (1 - cosine distance).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    user_query : np.ndarray\n",
    "        Embedding vector of the user query.\n",
    "    top_k : int, default=5\n",
    "        Number of items to retrieve.\n",
    "    df : pd.DataFrame\n",
    "        DataFrame containing: 'title' (str), 'receipt' (str), 'embedding_vector' (np.ndarray).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Columns: ['id', 'title', 'receipt', 'similarity']\n",
    "    \"\"\"\n",
    "    if df is None or df.empty:\n",
    "        raise ValueError(\"You must pass a DataFrame with embedded receipts.\")\n",
    "\n",
    "    # Compute similarity for each embedding vector\n",
    "    similarities = []\n",
    "    for _, row in df.iterrows():\n",
    "        emb = row[\"embedding_vector\"]\n",
    "        if isinstance(emb, np.ndarray) and emb.size > 0:\n",
    "            sim = 1 - cosine(user_query, emb)  # cosine similarity\n",
    "        else:\n",
    "            sim = -1  # placeholder for invalid rows\n",
    "        similarities.append(sim)\n",
    "\n",
    "    # Attach scores and sort\n",
    "    df[\"similarity\"] = similarities\n",
    "    df_sorted = df.sort_values(\"similarity\", ascending=False).head(top_k).reset_index(drop=True)\n",
    "\n",
    "    # Create consistent SQL-like view\n",
    "    result = pd.DataFrame({\n",
    "        \"id\": df_sorted.index,\n",
    "        \"title\": df_sorted[\"title\"],\n",
    "        \"receipt\": df_sorted[\"receipt\"],\n",
    "        \"similarity\": df_sorted[\"similarity\"]\n",
    "    })\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76cc7b3f",
   "metadata": {},
   "source": [
    "#### Test Similarity Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19e651f",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompt = \"\"\"\n",
    "Hi! I’d like to cook a good Italian dish for lunch! I have potatoes, carrots, \n",
    "rosemary, and pork. Can you recommend a recipe and help me a bit with \n",
    "preparation tips?\n",
    "\"\"\"\n",
    "\n",
    "resp = client.embeddings.create(        \n",
    "        model=model_name,                   \n",
    "        input=[user_prompt]                        \n",
    "    )\n",
    "user_query = resp.data[0].embedding\n",
    "\n",
    "prompt_recipes = rag_retrieve(user_query, top_k=5, df=embeddings)\n",
    "print(prompt_recipes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b36e22",
   "metadata": {},
   "source": [
    "### 5. AI Agent\n",
    "\n",
    "A tiny agent that:\n",
    "\n",
    "1) Plans & decides whether to use Google Search (tool calling),\n",
    "2) Always uses internal RAG first,\n",
    "3) Optionally augments with web results,\n",
    "4) Writes a final self-prompt and executes it,\n",
    "5) Logs every step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41e6405",
   "metadata": {},
   "source": [
    "#### 5.0 Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14caca40",
   "metadata": {},
   "outputs": [],
   "source": [
    "log = []  # each entry: {\"ts\": str, \"event\": str, \"data\": any}\n",
    "\n",
    "def _log(event, data):\n",
    "    log.append({\"ts\": datetime.utcnow().isoformat(), \"event\": event, \"data\": data})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5416c44d",
   "metadata": {},
   "source": [
    "#### 5.1 Retreival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51acdc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k = 5\n",
    "rag = rag_retrieve(user_query = user_query, top_k = top_k, df = embeddings)\n",
    "_log(\"rag.retrieve\", {\"top_k\": top_k, \"title\": rag[\"title\"], \"score\": rag[\"similarity\"]})\n",
    "display(rag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98d5710",
   "metadata": {},
   "outputs": [],
   "source": [
    "log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d6250d",
   "metadata": {},
   "source": [
    "#### 5.2 Execution Plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede46afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask the model to PLAN: Should we call Google Search?\n",
    "instruction = (\n",
    "    \"You are a planning assistant. Decide if web search is needed to improve answer quality \"\n",
    "    \"for the provided user question.\"\n",
    "    \"Return JSON with fields: need_search (true/false), search_query (string), rationale (string), \"\n",
    "    \"and then propose a short step-by-step plan for how you'll compose the final answer. \"\n",
    "    \"The RAG context needs to encompass A. five (5) recipes in order to be accepted as\"\n",
    "    \"strong and specific and B. all five (5) recipes must encompass \"\n",
    "    \"exactly the ingredients that are mentioned in the user questions.\"\n",
    ")\n",
    "user_plan = (instruction\n",
    "    + f\"### USER QUESTION ###: {user_prompt}\\n\\n\" \n",
    "    + f\"### RAG CONTEXT ###:\\n{rag['receipt']}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c882f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_plan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a2b55d",
   "metadata": {},
   "source": [
    "Produce execution plan:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e8211d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plan_resp = client.chat.completions.create(\n",
    "    model=\"gpt-4\",\n",
    "    messages= [{\"role\": \"user\", \"content\": user_plan}],\n",
    "    tools=tools,  # tools available if the model wants to call them later\n",
    "    temperature=0,\n",
    ")\n",
    "plan_text = plan_resp.choices[0].message.content\n",
    "_log(\"plan.draft\", plan_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b61b430",
   "metadata": {},
   "outputs": [],
   "source": [
    "plan = json.loads(log[1]['data'])\n",
    "print(plan[\"need_search\"])\n",
    "print(plan[\"search_query\"])\n",
    "print(plan[\"rationale\"])\n",
    "print(plan[\"plan\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6649a46",
   "metadata": {},
   "source": [
    "#### 5.3 Search, if necessary:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f29512",
   "metadata": {},
   "source": [
    "Prepare tool:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0b56bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tc = tools.copy()\n",
    "tc[0][\"function\"][\"parameters\"][\"properties\"][\"query\"] = plan[\"search_query\"]\n",
    "tc[0][\"function\"][\"parameters\"][\"properties\"][\"num\"] = 10\n",
    "tc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4351020",
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_outputs = []\n",
    "if plan[\"need_search\"]:\n",
    "    result = tool_dispatch(tc)\n",
    "    tool_outputs.append({\"name\": tc[0][\"function\"][\"name\"], \n",
    "                         \"args\": tc[0][\"function\"][\"parameters\"][\"properties\"], \n",
    "                         \"result\": result})\n",
    "    _log(\"tools.executed\", tool_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ce8c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0420cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "web_context = \"\"\n",
    "for item in result:\n",
    "    web_context += f\"Title: {item['title']}\\n\"\n",
    "    web_context += f\"Link: {item['link']}\\n\"\n",
    "    web_context += f\"Snippet: {item['snippet']}\\n\\n\"\n",
    "print(web_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71275ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_context = \"\\n\\n\".join(rag[\"receipt\"].astype(str).tolist())\n",
    "print(rag_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9a7739",
   "metadata": {},
   "source": [
    "### 6. Self-Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f6ddbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask the model to SELF-PROMPT: Should we call Google Search?\n",
    "instruction = (\n",
    "    \"You are a prompt engineer. Compose the best possible prompt for \"\n",
    "    \"a Large Language Model (LLM) \"\n",
    "    \"to answer the provided user question in the ### USER QUESTION ### section.\"\n",
    "    \" The ### RAG CONTEXT ### section provides results obtained from the \"\n",
    "    \"Retrieval Augmented Framework with similarity search in a vector database. \"\n",
    "    \" The RAG CONTEXT resuls might be augmented by Google Search results in the \"\n",
    "    \" ### WEB CONTEXT ### section.\"\n",
    "    \"Do not attempt to answer the user qestion; return only the prompt text. \"\n",
    "    \"Be systematic, be detailed, introduce sections, and precise instructions for an LLM \" \n",
    "    \"on how to answer the user question.\" \n",
    "    \"Assume that you have strings named user_prompt, web_context and rag_context in Python \"\n",
    "    \" encompassing the user question and everything that is found under ### RAG CONTEXT ### \" \n",
    "    \"and ### WEB CONTEXT ###\"\n",
    "    \"; produce your prompt as a Python string using user_prompt, web_context and rag_contex as variables in curly brackets.\" \n",
    "    \" Do not produce a prompt that asks the user for any interaction: explain the user question \"\n",
    "    \" to an LLM, provide the context, and instruct it how to help the user prepare a meal.\"\n",
    "    \" Remember: you are not about to answer to the user question. Your task is to produce a \"\n",
    "    \" prompt for another LLM to answer the user question.\"\n",
    "    \" Remember to use web_context and rag_context as variables in curly brackets in your final \"\n",
    "    \"response - a Python string.\"\n",
    "    \" You must liteary use the variable rag_context and the variable web_context in your output \"\n",
    "    \" ; place the variables in curly brackets in your output string\"\n",
    "    \"Make no introductions, just return the prompt as a string, with variables in curly brackets in it.\"\n",
    "    \" Do not attempt to answer the user question: your task is to instruct another LLM on how \"\n",
    "    \"to answer to it. Instruct the LLM to point towards the web resources (URLs) provided to it \" \n",
    "    \"in the web_context section.\"\n",
    "    \" Begin your prompt to another LLM with: The user is asking\"\n",
    ")\n",
    "user_plan = (instruction\n",
    "    + f\"### USER QUESTION ###: {user_prompt}\\n\\n\" \n",
    "    + f\"### RAG CONTEXT ###:\\n{rag_context}\\n\\n\" \n",
    "    + f\"### WEB CONTEXT ###:\\n{web_context}\" + \n",
    "    \"\"\"\n",
    "    ### OUTPUT FORMAT ### \n",
    "    - A plain string \n",
    "    - that is an instruction to another LLM and the answer to the user question,\n",
    "    - **always** using the variables named user_prompt, web_context, rag_context which **must be sorrounded by curly brackets** in your output string, \n",
    "    - **always** beginning with the words: The user is asking\"\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580d0e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_resp = client.chat.completions.create(\n",
    "    model=\"gpt-4\",\n",
    "    messages= [{\"role\": \"user\", \"content\": user_plan}],\n",
    "    temperature=0,\n",
    ")\n",
    "final_prompt = prompt_resp.choices[0].message.content\n",
    "_log(\"final_prompt\", final_prompt)\n",
    "print(final_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769d2afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_prompt = final_prompt.format(user_prompt = user_prompt, \n",
    "                                   rag_context=rag_context, \n",
    "                                   web_context=web_context)\n",
    "print(final_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855255c8",
   "metadata": {},
   "source": [
    "#### Execute the final prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23dc313c",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_resp = client.chat.completions.create(\n",
    "    model=\"gpt-4\",\n",
    "    messages= [{\"role\": \"user\", \"content\": final_prompt}],\n",
    "    temperature=0,\n",
    ")\n",
    "output = final_resp.choices[0].message.content\n",
    "_log(\"output\", output)\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aipy_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
